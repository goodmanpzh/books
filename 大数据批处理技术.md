# 大数据批处理技术

## 1. Hadoop集群简介

Hadoop的主要作用：

- 管理分布式集群
- 分布式存储数据
- 分布式处理数据

核心组件：

- HDFS：分布式文件系统，为上层应用提供数据存储的接口
- MapReduce：基于YARN的大型数据并行化处理系统
- Yarn：任务调度和集群资源管理的框架，为上层的应用计算（存储）分配资源

外部应用：

- Flume: 一个日志收集系统, 可以从不同的源端对大量日志数据进行收集、聚合、存储
- Sqoop: 用于在关系型数据库与Hadoop平台之间进行数据的导入导出工作
- Kafka: 一种高吞吐的分布式消息订阅系统
- HBase: 一种支持大型表的结构化数据存储的分布式数据库, 底层使用HDFS进行数据存储, 依赖Zookeeper进行集群协调服务
- Zookeeper: 一种高性能的分布式应用协调系统
- ElasticSearch: 一种分布式全文搜索引擎
- Hive:基于Hadoop的数据仓库工具, 将结构化数据映射为一张数据表, 支类SQL查询方式
- Storm: 一种分布式实时计算系统
- Spark: 一种快速通用的Hadoop计算引擎,提供简单且强力的编程模型, 支持
- ETL(Extract、Transform、Load)和机器学习、流处理、图形计算

四个优势：高可靠、高扩展、高性能、高容错

## 2. YARN集群架构

> YARN集群总体上是一个经典的主/从(Master/Slave)架构, 主要由ResourceManager、NodeManager、ApplicationMaster(下面简称AppMaster)和Container等组件构成.

组件介绍：

1. ResourceManager负责对集群资源进行统一管理和任务调度, 主要职责如下:

   - 管理NodeManager, 接收来自NodeManager的资源和节点健康汇报

   - 负责接收客户端的请求

   - 启动和管理各个应用程序的AppMaster

   - 接收来自AppMaster的请求, 并分配Container

     

2. NodeManager负责管理节点的资源和任务, 主要的职责是:

   - 定时向ResourceManager汇报本节点的资源(内存、CPU)使用情况, 以及各个Container的运行状态

   - 接收并处理来自AppMaster的Container启动/停止请求

   - 监视Container的资源使用情况, 如果某个Container资源使用超出界限则停止该Container

     

3. AppMaster是应用的程序管理者, 负责应用程序的管理, 主要职责如下:

   - 向ResourceManager申请运行应用的资
     源

   - 为应用程序的Task分配资源

   - 在Container中启动、重启、停止Task

     

4. Container: Task运行的容器, 封装了CPU和内存资源的“小电脑”, 是YARN集群中资源分配的基本单位. Container有两种, 一种用于运行AppMaster, 另一种运行Task.

   

5. Task: 应用程序具体执行的任务, 一个应用程序可以对应多个任务。

   

工作流程：

1. 客户端client向ResourceManager提交应用程序
2. ResourceManager会分配一个Container并请求NodeManager运行管理应用程序的ApplicationMaster
3. AppMaster会向ResourceManager注册自身信息, 并申请运行应用程序各个Task所需要的资源(Container), ResourceManager响应AppMaster, 分配Container.
4. AppMaster向主机上的NodeManager使用申请到的Container运行Task
5. Task运行时会向AppMaster汇报状态, 当Task运行失败时, AppMaster向NodeManager申请重启Task, 当Task运行完成时, AppMaster向NodeManager申请注销Task
6.  当所有Task运行完成, AppMaster会向ResourceManager申请注销自己

## 3. HDFS

> HDFS(Hadoop Distributed File System)是Hadoop项目的核心子项目, 在大数据开发中通过**分布式计算对海量数据进行存储和管理**. 他是基于**流数据模式访问和处理超大文件**的需求而开发, 可以**运行在廉价的商用服务器**上, 为海量数据提供了高可靠的存储方法.

设计目标：

1. 硬件故障检测：在超大的集群中, 硬件故障是常态, 而非特例, 因对于集群节点快速的故障检测、恢复是HDFS的核心架构目标.
2. 流式数据访问：在HDFS上运行的应用程序需要对其数据集进行流式访问, 因而, HDFS适合批量数据处理, 而不是与用户的实时交互, 因为HDFS更加强调数据访问的高吞吐量而非低延迟。
3. 大型数据集：HDFS支持大型数据集的存储(TB、PB、EB), 一个型的HDFS数据文件大小可以为千兆字节到百万兆不等, 而且, HDFS集群的数百个节点提供了高聚合的数据带宽和规模, 可以同时承载千万个文件.
4. 简单一致性：HDFS遵循简单的一致性模型, 一次写入, 多次读取. 一旦文件被创建, 写入, 关闭, 就不能被修改, 依此来保证数据的一致性. 在Hadoop2.x以后, 可以在文件末尾追加数据.
5.  移动计算比移动数据容易：HDFS为应用程序提供了接口, 可以移动应用程序的计算子任务到更加接近数据的位置, 而不是将数据移
   动到计算任务的位置.
6. 可移植性：HDFS是HADOOP的子项目, HADOOP是JAVA开发的, 因而可以轻松的移植到其他安装JAVA环境的平台

优缺点：

| 优点     | 缺点                     |
| -------- | ------------------------ |
| 高容错   | 不适合低延迟数据访问     |
| 可移植性 | 无法有效地存储大量小文件 |
| 大量数据 | 不支持并发写和修改文件   |

整体架构：

> HDFS架构是一个典型的主/从(master/slaver)架构的分布式系统, 整个集群有一个元数据节点(NameNode)和多个数据节点(DataNode)构成

- NameNode为主节点, 用于存储文件的元数据信息(文件名称、大小、位置)和处理来自客户端文件访问请求(Read/Write)的主服务器。

  它将所有的文件和文件夹的元数据保存在一个文件系统目录书中，任何元数据信息的改变，NameNode都会记录。HDFS中的每个文件都别拆分为多个数据块进行存放，文件和这些数据块的对应关系也会存放在目录树中。

  NameNode会周期性的接收来自集群DataNode的“心跳”和“块报告”，如果NameNode长时间未收到某个DataNode的心跳报告，那么这个DataNode被判定为宕机，NameNode负责将该DataNode的数据块分配到其他的DataNode。

- DataNode为从节点, 用于管理对应节点下服务器的数据存储, 实际数据分存储在DataNode中.是数据真正存储的地方，DataNode除了接受NameNode的指令对数据进行创建、删除、复制外，还会接受客户端对数据块的读写请求。

- SecondaryNameNode为辅助主节点, 辅助NameNode管理元数据。不是NameNode的备用，仅是NameNode的辅助工具。

  HDFS的元数据信息主要存储在两个文件中: fsimage和edits:
  fsimage是文件系统映射文件, 主要存储文件的元数据信息, 包含文件系统目录树、数据块的索引.
  edits是HDFS的操作日志, HDFS对文件的修改操作会存储在该文件中.
  它们位于$HADOOP_HOME/tmp/dfs/name/current下

  当NameNode启动时, 会将fsimage和edits进行合并, 得到最新的元数据信息,但是在繁忙的集群中, 大量的操作会让edits文件变得特别大, 那么下次NameNode的启动变得很慢, 而SecondaryNameNode可以协助NameNode管理edits文件, 以保证它的大小在一个合适的范围.

  1. SecondaryNameNode会通知NameNode不要往edits中写日志, 此时NameNode会新建一个edits.new文件, 并将日志写入edits.new

  2. SecondaryNameNode会通过http get方式获取fsimage和edits文件, 并将他们合并成新的元数据文件, 命名为fsimage.ckpt, 之后通过http post复制给NameNode
  3. NameNode将edits替换为edits.new, 将fsimage替换为fsimage.ckpt

- Block(数据块)：HDFS中的每个文件都是以数据块的形式存储的, 默认数据块的基本存储单位为128Mb(Hadoop1.x为64Mb)

  数据块的优点：

  1. 便于存储大型数据, 特别是数据量大于单个节点的存储容量时更显优势, 例如一个大型数据集有5T, 但是我们的单个节点只有2T, 集群总共有3个节点, 这时, 我们可以将大型数据集切分成数据块, 然后每个节点存储一部分数据块, 这样整个数据集就存入了集群中;
  2. 为Hadoop高容错和高可用的实现带来方便, 存储整个大数据集时, 如果存储的节点出现异常, 那么数据集的全部数据可能丢失, 但是使用数据块来存储, 顶多丢失这个数据集在节点上的一部分数据 (高容错), 重启节点后, 我们将这些数据块恢复起来也更快吧(高可用);
  3. 数据读取效率更高, 如果整个数据存储的话, 我们采集数据时, 需要将全部数据传输过来.但是使用数据块的话, 可以只传输需要的数据块.

  为什么用128Mb作为存储单位：

  1. **减少硬盘寻道时间(disk seek time)**: 合适的块大小有助于减少硬盘寻道时间，提高系统吞吐量. 一般来说, 寻址时间占数据传输时间的1%最为合适, , 这时传输过程最有效率.而正常情况下的寻址时间为10ms, 磁盘(一般指机械硬盘)传输速度为100Mb/s, 所以设置为128Mb最为合适.
  2. **减少Namenode内存消耗(memory cost)**:每个数据块都会在NameNode上内存保存一条元数据, 元数据的大小150B的字节, 如果块大小设置过小, 同等数据量会占用更多的NameNode内存.**间接减小了集群的存储容量**. 并且过多的数据块也不利于NameNode维护整个集群.
  
- HDFS文件集群读写：

  读操作：

  1. 客户端向NameNode发送读请求，NameNode会查看存储在内存中的元数据信息
  2. NameNode向客户端返回目标数据的元数据信息（包含目标数据的数据块名称以及所在的DataNode）
  3. 客户端向DataNode请求读取数据块的数据，DataNode以数据流的形式将数据发送给客户端

  写操作：

  1. 客户端向NameNode发送写请求，客户端会将文件名、文件大小告诉NameNode. 然后NameNode会记录这些信息, 并验证客户端的权限, 如果通过才进行下一步.
  2. NameNode通过查找集群的信息, 将能够存储数据的DataNode以及存储位置返回给客户端.
  3.  客户端会请求相应的DataNode写入数据, 数据按块写
     入磁盘, 之后DataNode会将写入的数据块复制给其他DataNode以完成数据冗余操作.

## 4. MapReduce

> 在分布式集群中, 数据集一般都非常大, 以往的方式是将数据集移动到一台机器计算. 然而, 数据移动的时间成本高, 单一机器计算效率低下. 因此, 需要一个分布式的数据处理引擎, MapReduce就诞生了.
> MapReduce能够将计算任务分配到数据所在的节点, 节省数据移动的时间消耗, 并且利用多台机器并行地执行任务.

设计思想：

从编程角度看是MapReduce使用了map和reduce实现分而治之.

- **Map是映射，负责数据的过滤分发**, 在MapReduce并行计算中, 大型数据集被划分成小数据集计算, 采用的是map()方法, 各个map()方法对输入的数据进行并行处理,不同的输入对应不同的结果;
- **Reduce是规约，负责归并数据计算结果**, 合并小型数据集的计算果, 采用reduce()方法, reduce()也各自并行计算.reduce()的输入是map()的输出, 在输入至reduce()之前, 需要等待对应的map()方法执行完毕.

- **Shuffle是整合, 负责将无规则的输出形成一定规则后输入(对map的结果进行整理并发送给reduce)**, shuffle包含很多步骤partition、spill、merge等.

  shuffle的作用：减少了从map端拉取数据到reduce端的带宽消耗；减少了磁盘IO对map任务的影响

任务流程：

MapReduce使用YARN作为集群资源管理系统, 因此MapReduce的任务流程就是YARN的工作流:

1. 客户端向ResourceManager提交应用程序

2. ResourceManager在NodeManager的Container中启动MRAppMaster(MapReduce ApplicationMaster)
3. MRAppMaster向ResourceManager注册并申请资源
4. MRAppMaster向NodeManager请求在对应的Container中运行Map Task或者Reduce Task
5. Task向MRAppMaster汇报自身进展, 如果全部执行完成，MRAppMaster向ResourceManager申请注销自己

工作细节：

- Map阶段：
  1. 大数据集切片(split)的数量=大数据集所占的block数量=map任务的数量
  2. 每个map任务会将数据分块按行解析成多个<Key,Value>对, Key为该行起始位置, 一般为行号, Value为按一定规则解析出来的值, 一般为一行的内容.
  3. map任务一次只能处理一个<Key, Value>对, 每次处理后, 输出的仍然是<Key, Value>对. 输出的<Key,Value>类型可以自定义.
- Shuffle阶段：
  - spill(溢写): Map任务输出的<Key, Value>对首先会存放在任务所在节点的内存缓冲区中(默认为100MB), 当缓冲区的使用率达到默认阈值(默认为0.8)后便会将数据溢写到磁盘中.
  - partition: 在数据溢写到磁盘前, 需要对数据进行分区(partition), 分区的大小为Reduce任务大小(默认为1), 每个Reduce任务可以处理一个分区的数据,partition可以防止有的Reduce任务处理的数据过少,从而可以负载均衡, 避免数据倾斜. 分区算法为:partitionId = hashCode(Key) % num_partition,也就是对每个<Key, Value>对的Key取Hash值, 然后与分区数量取余, 得到的结果就是<Key, Value>对
    所属分区ID.
  - merge(合并): Reduce任务在载入数据前, 需要对每个“小分区”的数据进行合并, 合并后组成一个大分区, 然后对大分区的<Key, Value>按照Key进行排序. 每个分区会将排序后的<key, value>分组,然后再次合并成<Key, Value-List>
- Reduce阶段：
  - Reduce任务接收的是Shuffle后的<Key, Value-List>, 然后经过处理, 输出为<Key, Value>. 最终照“<Key>\t<Value>”存储在HDFS文件中
- 在编写MapReduce程序时, map()和reduce()需要用户根据业务实现, 根据需求还可自定义
  partition()函数, 至于其它如何从HDFS中读取数据到map(), 如何将数据从shuffle输入到reduce(),
  又或是如何将reduce的结果输出至HDFS中, 这些细节MapReduce已经帮我们定义好了.

## 5. Zookeeper

> Zookeeper是分布式的应用程序协调系统, 主要是为**了解决在分布式系统中应用系统的一致性问题.** 它使用类似于HDFS文件目录树的**目录节点树**来存储数据, 用于维护和监听所存数据的变化, 以实现对集群的管理.

在分布式集群中, 通常包含数以百计的节点, 一旦部署在其上的分布式应用的配置信息发生改变, 就需要修改每个节点的配置文件。做法：

| 方法                                                         | 特点                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 通过⼈⼯⼿动修改每个节点对应的配置⽂件                       | 缺点: 当集群服务器较多时将耗费⼤量的时间和⼈⼒成本.          |
| 存储在数据库中(例如Mysql): 将配置信息存储在数据库中, 服务器的应用读取配置信息. | 可靠性差, 当数据库挂掉后, 所有配置信息丢失, 服务器的应⽤不可运⾏. |
| 将配置信息存储在Zookeeper集群的节点上, 集群中的服务器通过应用监听该节点, 当节点数据更新后, 应用指示服务器更新配置文件. | 自动同步, 高可靠.                                            |

Zookeeper的应用场景：

- 统一命名服务：

  1. 根据Zookeeper的节点目录树结构, 可以将系统中各种服务的名称、地址以及目录信息存放在Zookeeper中. 需要时就可以去读取.
  2. 在分布式集群中, 很多生成序号的算法失效(如Mysql的主键生成算法), 而利用Zookeeper的顺序节点可以生成有顺序的分布式ID, 方便集群的应用使用.

- 分布式锁：

  在分布式系统中, 为了提高可靠性, 需要在集群的每个节点上都要运行相同的服务, 他们相互之间需要协调资源, 编程起来特别复杂. 我们可以**利用Zookeeper的分布式锁来协调分布式进程的资源,保证同时只让一个服务进行, 当服务完成或出现问题后立马切换到下一个服务.**

- 集群管理：

  在由多台服务器组成的集群中, Zookeeper可以非常容易地实现集群管理的功能:

  1. 首先, 在集群启动时, Zookeeper会通过选举机制选举出一个Leader, Leader能够知道当前集群中所有服务器的健康状态.
  2. 其次, 当集群中新增服务器、或者服务器宕机时, Leader能第一时间会通知整个集群的所有节点, 并做出相应的应对措施.
  3. Leader服务器发生故障而失效时, Follower们快速响应, 通过Zookeeper默认的FastLeaderElection机制投票选取新的Leader.

架构原理：

**Zookeeper也是一个典型的主从架构, Zookeeper集群由一组节点或服务器(Server)组成, 这些服务器中有一个称为Leader(主节点), 其他的节点称为Follower(从节点).**
当客户端连接Zookeeper的任意一个节点并发送读写请求时, 首先这些请求会发送给Leader, Leader执行这些请求(例如向节点目录树中写入数据), **Leader会首先将这些数据持久化本地磁盘, 然后再将数据变更操作写入内存中**. 最后Leader会将这些数据变更同步到其他节点.

数据类型：

- 命名空间：**Zookeeper有一个类似于HDFS文件目录树的节点目录树, 也称为命名空间. **Zookeeper的命名空间存储在内存中, 因此可以很方便快速地将这个命名空间共享给其他的分布式应用程序(例如HBase、Kafka), 让它们也可以很方便地利用命名空间协调集群.

- 节点：在命名空间中, 名称由斜线(/)分割的路径元素组成. 命名空间的每个名称(也称为节点)都由路径标识. 通常使用znode来表示 .

  可以将命名空间类比于一个多叉树, 整个多叉树就是一个命名空间, 多叉树的每个节点就是命名空间的节点,多叉树也有根节点、父节点、子节点. 

  在命名空间中也有这些概念.命名空间的根节点的路径标识为“/”. 父节点指的是某个节点的上一层节点, 子节点指的是某个节点的下一层节点.

  每个znode可以有与之相关联的数据以及子节点, 例如在HDFS中, 每个文件夹下都可以存储文件或者文件
  夹.

  每个znode有两种数据: **主体数据**(例如配置信息、集群参数等)和**状态数据**(描述节点状态信息的数据). 前者由客户端可以进行修改, 后者由Zookeeper集群自行维护

  znode的特点：

  - 存储协调数据(即集群同步相关数据), 例如状态信息、配置内容、位置信息等, 因此数据量很小, 一般B或者KB量级, 并且znode不适合存储大量数据集.
  - **znode维护一个自身状态结构(状态数据), 包含版本号, ACL(访问控制表, 可以限制客户端对该节点的访问权限)变更, 时间戳等.** 每次当前znode的数据发生改变时, 版本号就会递增, 客户端可以根据版本号索引数据.
  - 客户端可以在每个znode上设置监听程序(Watcher), 一旦当前节点数据发生改变, 例如节点主体数据改变、节点删除、子节点创建或删除, Zookeeper就会通知客户端做出相应的应对措施(通过一个回调函数实现).

节点类型：

  > Zookeeper的节点分为4种: 持久节点、持久顺序节点、临时节点、临时顺序节点. 正是通过这些特殊的节点。Zookeeper才能实现前面所提的配置管理、统一命名、分布式锁和集群管理等功能.

  - 持久节点(PERSISTENT)
    客户端创建持久节点后, 当客户端断开连接时, 节点仍然存在. Zookeeper命令行默认创建持久节点.
  - 持久顺序节点(PERSISTENT_SEUENTIAL)
    持久顺序节点除了有持久节点的特性外, 在创建时, Zookeeper会在节点名称后面自动追加一个自增长的后缀, 以便记录节点的创建顺序. 这个后缀由10个数字构成, 由0填充, 从0000000000开始.
  - 临时节点(EPHEMERAL)
    与持久节点相反, 客户端创建临时节点后, 当客户端断开连接时, 节点消失. 基于此, 临时节点不允许存在子节点.
  - 临时顺序节点(EPHEMERAL_SEQUENTIAL)
    临时顺序节点除具有临时节点的特性, 在创建时, 还会在节点名称后面追加自动增长的数字编号, 这点与持久顺序节点相同.

Zookeeper的Watcher机制：

> Zookeeper允许客户端在znode上注册一个Watcher, 当znode的状态发生改变时(节点创建、节点删除、修改元数据、子节点变更), Zookeeper会通知在该节点上注册的Watcher做出相应的操作.

- Watcher机制是一次性的，触发之后不在触发，除非重新注册Watcher
- Watcher机制是异步的，在触发之前就设置好，触发之后再起作用

Zookeeper的运行流程：

1. 客户端向Zookeeper集群的某个znode注册监听机制(Watcher)
2. 同时,客户端会将Watcher对象存储在客户端的WatchManager中
3. znode发生改变, Zookeeper触发Watcher并向客户端发送通知
4. 客户端从WatchManager中取回相应的watcher对象, 并执行回调函数.

Zookeeper的分布式锁：

> 在分布式环境中, 为了**保证在同⼀时刻只有⼀个客户端对指定数据进⾏修改**, 需要利⽤分布式锁技术, 只有获得锁的客户端才能访问数据, 其余客户端只能等待(类⽐于mysql数据库的⾏锁、表锁). 利用Zookeeper可以轻易地实现分布式锁.

1. 所有客户端连接Zookeeper, 调⽤create()函数在指定的锁节点(如“/lock”)下新建⼀个临时顺序节点.假设名称为”node-”, 共有10个客户端, 它们新建的节点名称为“node-0000000000”, …,“node-0000000010”.

2. 每个客户端调⽤getChildren()⽅法获取”/lock”的⼦节点, 判断序号最⼩的⼦节点是否为⾃⼰创建, 如果是, 则⽴即获得锁; 否则, 监听序号⽐⾃⼰⼩⼀号的⼦节点;
3. 获得锁的客户端执⾏业务代码
4. 客户端完成任务或宕机后断开与Zookeeper的连接,由于客户端创建的是临时(顺序)节点, Zookeeper⽴刻删除该客户端的⼦节点(释放锁).
5. 删除⼦节点操作会产⽣NodeDeleted的事件,Zookeeper通知监听在该节点上的客户端. 回到第3步, 直到所有节点被删除.

创建临时顺序节点的作用：

1. 客户端创建的节点为临时节点, 这样可以**保证客户端在任务完成或者宕机后能够及时释放锁**
2. 每个节点监听序号⽐⾃⼰⼩⼀号的节点, 保证客户端按照节点序号从⼩到⼤的运⾏下去, 并且每次监听事件触发后(节点删除), **Zookeeper只需通知⼀个客户端, 节省了⽹络带宽.**

## 6. HBase

> HBase是一个开源的非关系型数据库, 它的原型是谷歌的BigTable论文, 目前作为Hadoop的顶级子项目来开发维护, 它可以存储结构化数据. 与BigTable不同的是:
>
> - 使用HDFS作为可靠的底层存储
> - 使用Hadoop MapReduce作为高性能的计算引擎
> - 使用Zookeeper来保证集群的稳定性和容灾性

HBase的优点：

- 海量数据：

  在数据量上, HBase适合存储PB级别的海量数据，在PB级别的数据以及采用廉价PC存储的情况下，能在几十到百毫秒内返回数据. 在表的结构上, 一个HBase表可以有数亿计的行和数以百万计的列, 存储大量数据时无需分表, 查询数据非常方便.

- 扩展性强：

  HBase的扩展性主要体现在两个方面:一个是基于上层处理能力(RegionServer)的扩展，通过横向添加RegionServer的机器，进行水平扩展，提升HBase对上层业务的处理能力.一个是基于存储的扩展(HDFS),过横向添加DataNode的机器，进行存储层扩容，提升HBase的数据存储能力和提升后端存储的读写能力.

- 列式存储：

  与很多⾯向⾏存储的关系型数据库不同，HBase是根据列族来存储数据和管理权限. 它⾥⾯的每个列族是单独存储的，且⽀持基于列的独⽴检索。

  行存储里的一张表的数据都放在一起，但在列存储里是按照列分开保存的.

  在这种情况下，进行数据的插入和更新，行存储会相对容易.

  而进行行存储时，查询操作需要读取所有的数据，列存储则只需要读取相关列，可以大幅降低系统 I/O 吞吐量.

- 高并发：

  在并发的情况下，HBase的单个IO延迟下降并不多。能获得⾼并发、低延迟的服务.

- 稀疏性：

  通常在传统的关系性数据库中，每⼀列的数据类型是事先定义好的，会占⽤固定的内存空间，在此情况下，属性值为空（NULL）的列也需要占⽤存储空间
  HBase 中的数据都是以字符串形式存储的，为空的列并不占⽤存储空间，因此 HBase 的列存储解决了数据稀疏性的问题，在很⼤程度上节省了存储开销

总体上说，HBase是一种分布式、可扩展、支持海量数据存储的非关系型列式数据库。

HBase的基本结构：

1.  命名空间(Namespace)
   命名空间，类似于关系型数据库的DataBase概念，每
   个命名空间下有多个表。
   HBase有两个自带的命名空间，分别是hbase和 default，Hbase中存放的是HBase内置的表， default表是用户默认使用的命名空间。

2. 数据表

   在HBase中, 数据存储在数据表中, 表由⾏和列构成,与关系型数据库(RDBMS)不同, HBase表是多维映射的。

3. 行

   HBase表中的每行数据都由一个行键(RowKey)和多个列(Column)组成，数据是按照RowKey的字典顺序存储的,并且查询数据时需要根据RowKey进行检索. 所以 RowKey的设计十分重要, 利用行键的特性可以将相关的数据排列在一起, 提高查询效率。

4. 列族

   HBase的列族是由多个列组成, 相当于对列进⾏分组,列的数量没有限定, 每个列族所属列可达百万级.
   在创建数据表时, 需要指定列族, 指定后不能轻易修改,且数量不宜太多, ⼀般不超过3个.

5. 列限定符

   列限定符代表HBase中列的名称, 列族中的数据通过列限定符定位, 通常格式为”column family:qualifier”(如,personal_info:name、office_info:tel)
   列族和列限定符都可以理解为列, 只是级别不同, 列族为一级, 列限定符为二级, 两者为父子关系.
   在插入数据时, 可以动态的为列族增加列限定符.

6. 单元格

   单元格可以通过列族和列限定符⼀起定位, 单元格包含时间戳和值, 该时间戳代表值的版本, 默认情况下, 时间戳表示系统的时间. 值没有类型, 总是视为字节数据byte[].
   当插⼊数据时, 我们可以指定不同的时间戳
   每⼀个单元中保存着值的多个版本, 且按照时间戳降序排列, 这样, 我们每次查询数据默认获得最新版本。

HBase的集群架构：

整体上, HBase是一个经典的主从(master/slave)架构, 集群由三种节点组成: HMaster节点, HRegionServer节点和Zookeeper集群. HMaster作为主节点, HRegionServer为从节点, 这种主从架构类似于HDFS的集群架构.

Zookeeper用于协调HBase集群的所有节点, 以保证任意时刻只有一个HMaster节点 , 同时, HMaster也是通过Zookeeper对集群所有HRegionServer的状态进行管理.

局部组件：

- HMaster不存储HBase的任何数据, 它只是管理集群所有的HRegionServer,并且将一张表的数据按行切分成HRegion存储在HRegionServer中.
- HRegionServer管理自身的HRegion并且将HRegion按照列(族)分成多个Store.
- Store由一个MemStore和多个HFile组成.
- HFile为HBase的底层存储格式. 最终数据以HFile的形式存储在HDFS中.

组件介绍：

- HMaster：

  >  HMaster可以同时启动多个, 但是同一时刻只能有一个激活. HMaster掉线后, Zookeeper通过选举重新激活新的HMaster.

  HMaster的作用如下:

  - 管理HRegionServer节点, 指定HRegionServer可以管理哪些HRegion, 实现负载均衡;
  - 当某个HRegionServer宕机后, 可以将该HRegionServer负责的数据分配给其他HRegionServer, 实现高可用;
  - 接收客户端的增删改查操作;
  - 管理数据表的元数据信息, 保存HRegion和HRegionServer的映射关系;
  - 权限控制, 管理用户对表的权限.

- HRegionServer

  HBase自动按行(rowkey)将数据表切分成多个区域, 每个区域称为HRegion. HRegionServer管理HRegion与响应客户端对HRegion的操作请求.
  最初, 一张数据表仅有一个HRegion, 但是当行的数量增加到一定量时(例如增加到数千万行), 此时HBase自动均匀地将数据表切分成两个HRegion并分配给不同的HRegionServer. 因此, 一张表可能包含多个HRegion, 也可能存储在不同的节点上.
  每个HRegion记录着内部所有数据的开始行键(startkey)和结束行键(stopkey). 因此, 客户端可以通过HMaster节点快速地定位每个rowkey所在的HRegion.

- Store

  每个HRegion又按照列族切分成很多区域, 每个区域称为Store. 一个HRegion可能包含多个Store.
  Store一个MemStore和多个HFile. MemStore相当于内存缓冲区, 数据存入磁盘前会先存入MemStore中, 当MemStore积累到一定大小时会持久化(Flush)为一个HFile. StoreFile是HFile的封装,
  HFile是HBase的底层存储单元, 最终数据也是以HFile的形式存储在HDFS中.
  一行数据也可能存储在多个HFile中.

- HLog

  HLog是HBase的日志文件, 记录这数据的更新操作.
  HBase在将数据写入MemStore之前, 会将操作记录在Hlog中, 然后才会将数据写入MemStore, 只有两个操作都完成时, 才被认为数据写入成功. 这个过程称为WAL(预写日志).
  WAL的作用是为了保证数据的一致性和实现回滚操作, 例如在插入数据时服务器崩溃了, 那么MemStore(内存)中的数据丢失, 而HLog存储在HDFS中, 服务器崩溃时Hlog仍然可用, 所以可以通过HLog来恢复丢失的数据

- Zookeeper

  HMaster使用Zookeeper监听HRegionServer的健康状态. 每个HRegionServer都在Zookeeper上注册一个临时节点, HMaster通过这些临时节点监听可用的HRegionServer (/hbase/rs)
  Zookeeper通过分布式锁保证同一时间只有一个HMaster处于激活状态 (/hbase/backup-masters)
  HMaster通过Zookeeper来确定HRegion应该分配给哪些HRegionServer

















## 常用基本命令

运行hadoop jar 可以直接在hdfs文件系统上进行操作。

hadoop jar [jar文件位置] [jar主类] [HDFS输入位置] [HDFS输出位置] 

### 1. linux基本命令

![linux](.\bigdata\linux_命令.png)

### 2.HDFS命令

![HDFS](.\bigdata\hdfs1.png)

![HDFS2](.\bigdata\hdfs2.png)

![HDFS2](.\bigdata\hdfs3.png)

![HDFS2](.\bigdata\hdfs4.png)

### 3. Zookeeper命令

![zookeeper](.\bigdata\Zookeeper.png)
