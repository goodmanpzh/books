# 爬虫学习笔记:cow:



## 1. requests包

requests 包主要是用来进行网页内容的抓取，一共可以分为两种方式，一种是get，一种是post。

### 1.1  `requests.get()`请求的使用

get主要用来进行网页源代码的获取，其中需要的参数如下：

`requests.get(url,params,headers,timeout,**kwargs)`

- url：需要爬取的网址，其中可以是网址头部地址，也可以是浏览器抓包工具中的地址。
- params：可选参数，其内容为字典形式，网站中需要的其他参数，包括页数信息等参数。
- headers：以字典形式存放，其中最主要的就是user-agent和cookie参数，用来伪装成人的操作，当网站无法进行读取时，首先考虑的就是加上这两个参数。
- verify：参数值为True或False，适用网站: https类型网站但是没有经过 证书认证机构 认证的网站。适用场景: 抛出 SSLError 异常则考虑使用此参数

requests函数的返回值为一个response类的对象，如果直接输出的话会返回该对象的响应状态码，一般返回200即代表成功，其他返回值需要进一步进行分析处理，比如加上use-agent等参数。关于response对象的常用属性如下：

- encoding：`response.encodng='utf-8'`,用来将得到的网页结果进行转码。
- text：`response.text`，响应的内容，一般用来查看返回的网页源代码或抓取网页的信息。会自动根据网页信息进行转码，如果已使用`response.encoding`,则会根据手动转码的规则进行解码。可以用来获取文本信息。
- content：`response.content`,二进制的响应内容，字节方式进行存储，主要用来获取图片或视频。
- `json()`：`response.json`,可以获取网页得到的字典形式的内容。

### 1.2  `requests.post()`请求的使用
post与get区别主要是post大多是在浏览器端渲染的，使用post或get主要取决于浏览器抓包工具的响应值。并且post与get的参数大同小异：

`post(url, data=None, json=None, **kwargs)`

- url：调用接口的url地址。
- data：可选参数，参数类型是一个字典类型，在浏览器抓包工具中会有具体体现，其中包括页数、搜寻的标签等内容。
- headers：可选，但是大多数的post请求都需要添加这一信息，以及cookie等参数。

- text：响应内容。字符串方式，会自动根据响应头部的字符编码进行解码。如果你改变了编码r.encoding，每当你访问 r.text ，request 都将会使用 r.encoding 的新值。
- content：二进制响应内容。字节方式，会自动为你解码gzip和deflate压缩。
- raw：原始响应内容，也就是 urllib 的 response 对象，请求中要加stream=True，再使用 r.raw.read() 读取。

### 1.3 requests的使用过程

1. 导入包 `import requests`
2. 创建所要请求网站接口对象：`url = 'https://www.baidu.com/'`，网站可以用含有参数的方式，进行爬取多个类似的网页。`url = url+'s'`
3. 创建参数：`data = {page,text,kw}、headers = { cookie,user-agent}`
4. 调用requests包，生成response对象：`resp = requests.get/post(url = url [headers = headers,data = data，verify = False])`,其中括号内的为可选的参数。
5. 对得到的对象进行重新编码。该步骤不是必须的，当自动解码出现问题时，可以用`resp.encoding = 'utf-8'/'gbk'`进行重新编码。
6. 打印预览返回得到的结果内容。`print(resp.text/resp.json())`,将得到的结果进行储存以进一步对其进行解析。`resp_text = resp.text`。
7. resp.close()：养成良好的习惯，在抓完后关闭接口。

### 1.4 注意事项

1. 在进行循环爬取时，要注意睡眠1s，不然可能会出现错误，可能会被封ip。



----



## 2. re模块-正则表达式

在从request或者selenium等方式获得网页代码后，提取所需要的信息是很重要的一个模块，因为网页的源代码中的信息非常杂乱，但是它们之间又是以一种复方的形式进行加载的，所以采用特殊的方式可以进行有用信息的提取。

### 2.1 正则匹配

[正则匹配](https://blog.csdn.net/sinat_34626741/article/details/123883244)

> .  匹配除换行符以外的任何字符
>
> \w 匹配任意的字母或数字或下划线
>
> \d 匹配数字
>
> 贪婪匹配（尽可能多的匹配）：  .*
>
> 惰性匹配（最大程度上匹配最少的相同内容）

最常用的匹配规则是惰性匹配。

### 2.2 re模块的使用

假设已经得到了由requests获得的网页源代码等资源为resp_text。需要进行有效信息的提取，re模块可以利用正则表达式来很好地完成这一工作。re的工作流程如下：

1. 导包：`import re`
2. 预编译正则表达式：`obj = re.compile( r"正则表达式内容"，re.S)`,其中正则表达式内容可以用以下方式进行快速编写:

- 复制出想要提取的内容，将提取的内容的共有部分保留，并且这部分内容可以唯一标识想要提取的部分，在提取部分加上`（？P<name>.*?）`。其中  .*?  就是利用惰性匹配进行匹配，（？P<name>）代表这部分是要提取的内容，name可以随便起，用来作为index。

3. 在编译过后就可以对象要的部分进行提取，这时，obj就是一个Pattern对象，可以通过Pattern对象提供的一系列方法来对文本进行匹配和查找。所包含的常用方法有：

   |   方法  |   说明   |
   | ---- | ---- |
   | `obj.search()`  |   从给定的文本中寻找，找到就返回   |
   |   `obj.findall()`   |   找到所有的匹配项，将匹配到的结果返回到一个列表中，如果没有就返回空列表，如果有多个分组，就会将一个匹配中的所有分组内容组成一个元组，在将所有的元组写到一个列表里。   |
   | `obj.finditer()` | 找到的内容以一个迭代器的形式存储，最常用的一种方法，可以将得到的内容进一步循环进行其他处理。 |

4. 常用保存得到结果的部分代码：

```python
import csv
obj = re.compile(r'正则表达式',re.S)

#对内容进行匹配
result = obj.finiter(resp.text)
print(result)
# 得到的结果的每一项都有相应分组的值，可用i.group('name')表示,其中i是下面的i

#将得到的内容进行写入
with open("output/veg.csv",mode='a+',encoding="utf-8") as f:
    csvwrite = csv.writer(f)
    for i in result:
        dic = i.groupdict() #将结果取字典 
        csvwrite.writerow(dic.values()) #保存字典中的values

```

### 2.3 注意事项

1. 注意编译正则表达式时，要加上re.S,否则在可能会出现表达是正确却找不到，该参数是将 . 赋予能够匹配换行或者空格的含义。
2. 在存入文件的过程中注意编码格式等内容。
3. 在进行嵌套使用re来进行正则匹配时，匹配到的第一大部分包含需要的其它小部分，可以使用迭代器finditer，然后再用嵌套循环，外部循环1次，内部将所有需要的匹配利用内部循环都找出来，注意两个正则表达式的使用，不要弄混。



-----

## 3. BeautifulSoup

> 简单来说，BeautifulSoup 就是 Python 的一个 HTML 或 XML 的解析库，我们可以用它来方便地从网页中提取数据，官方的解释如下：
>
> BeautifulSoup 提供一些简单的、python 式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。BeautifulSoup 自动将输入文档转换为 Unicode 编码，输出文档转换为 utf-8 编码。

BeautifulSoup同样可以用来进行网络爬虫的数据解析工作，使用方式如下。

### 3.1 BeautifuSoup使用过程

> 前提：已经获得网页内容为 resp.text

1. 导包：`from bs4 import BeautifulSoup`
2. 创建soup对象：`soup = BeautifulSoup(resp.text,'lxml')`。其中，resp_text是html文档，其中的内容是网页的源代码。而参数'lxml'是解析器，不同的解析器有不同的优点，也可以使用python自带的解析器'html.parser'。解析器会自动将网页中缺少的节点进行补齐，并生成一个格式化后的网页内容。可以使用`print(soup.prettify())`进行查看。
3. 根据特定的网页标签寻找指定的内容：其中查找有多种方式，具体的解析在后面的代码块中。
4. 得到想要获取的标签内容或者属性内容或者文本内容。

### 3.2 代码解析实例

```python
from bs4 import BeautifulSoup

html = '''
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title" name="dromouse"><b>The Dormouse's story</b></p>
<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1"><!-- Elsie --></a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>
<p class="story">...</p>
'''

soup = BeautifulSoup(html, 'lxml') #使用lxml解析器

#以下是简单浏览结构化数据的方法

# title标签 打印的是标签字符串
print(soup.title)   # 打印：<title>The Dormouse's story</title>

# 打印的是title标签里的内容
print(soup.title.string)   # 打印：The Dormouse's story

# p标签里面的class对象的名
print(soup.p['class'])      # 打印：['title']

# 打印标签的名称
print(soup.title.name)      # 打印：title

# .parent是选择父标签
print(soup.title.parent.name)   # 打印：head

#  打印所有的a标签字符串
print(soup.find_all('a'))


# 打印id = "link3" 的标签字符串
print(soup.find_all(id = "link3"))

# 从文档中找到所有a标签的链接
for i in soup.find_all('a'):
    print(i.get('href'))

# 获取所有的文字内容
print(soup.get_text())

# 获取class为story中的所有的a标签，返回的是所有a标签组成的列表
story = soup.find('p',class_ = 'story').find_all('a')
print(story)

# 从story中获取所有的a标签的href链接
for i in story:
    print(i.get('href'))
```

### 3.3 注意事项

1. 注意find和find_all的区别，find()只打印输出第一个找到的符合的标签，而find_all()是找到所有的符合条件的标签。
2. 注意在使用find函数填写属性内容时，class等是python的关键字，所以要加一个下划线class_才能正常运行。当寻找的标签只有一个时，属性值可有可无。
3. 获取所有的文字内容为get_text(),是一个非常有用的东西！



-----

## 4 xpath模块

> XPath，全称 XML Path Language，即 XML 路径语言，它是一门在XML文档中查找信息的语言。XPath 最初设计是用来搜寻XML文档的，但是它同样适用于 HTML 文档的搜索。所以在做爬虫时，我们完全可以使用 XPath 来做相应的信息抽取。

xpath使用时尽量结合浏览器的检查功能，再结合selenium会爽到起飞！

### 4.1 使用过程

> 前提：已经获得网页内容为 resp.text

1. 导包：from lxml import etree
2. 生成xpath解析对象：用`html = etree.HTML(resp.text)`，生成的解析对象就是完整的网页内容，会自动填补缺少的内容。可以用一下代码输出：

```python
html = etree.HTML(text)
result = etree.tostring(html)
print(result.decode('utf-8'))
```

3. 利用xpath路径表达式得到想要的网页源代码块或源代码的内容。其中获取到的内容可以有以下几种：

   | 表达式                                       | 获取内容                                                     |
   | -------------------------------------------- | ------------------------------------------------------------ |
   | divs = html.xpath('//*[@id="__layout"]/div') | 获取根目录下id为_layout的所有div标签，可以利用这些标签进一步做处理。 |
   | title = div.xpath('./a/text()')[0]           | 获取当前节点下，a标签中的文本内容                            |
   | result = html.xpath('//li/a/@href')          | 获取所有li标签下的a标签的href属性的内容。可以用来提取子页面的网址信息。 |

4. 注意所提取到的内容都是放在列表里面的，所以如果要保存或者打印输出，需要将提取到的信息加上[0]，即输出列表的第一项，得到得内容才是想要的字符串。

### 4.2 注意事项

1. 可以利用浏览器的检查中的复制xpath路径来迅速地提取想要的内容，比较适合网页内容清楚、格式规范的数据。



-----

## 5. requests进阶

### 5.1 利用会话session解决cookie

在有些需要登录才能查看具体信息的网页，如果每一次请求使用requests的话，就需要重新登陆网站，如果再次通过requests来获取子页面的时候，如果没有之前的登录内容就会出现问题。可以使用session来模拟持访问，session会自动记录cookie，然后再次访问时可以直接进行访问。

使用代码

```python
import requests

# 创建sess对象，使用会话功能抓取
sesssion = requests.session()

#在登录时，从浏览器抓包，得到login包中的内容
url = "https://passport.17k.com/ck/user/login"

#得到data
data = {
    'loginName': '15603372757',
    'password':'p2667459138'
}

#获取抓包结果
resp = sesssion.post(url,data = data)

# print(resp.text)

resp.close()

#查看cookie
# print(resp.cookies)

#进行子页面访问时，由于session中已经记录了登录的cookie，可以直接进行访问
url1 = "https://user.17k.com/ck/author/shelf?page=1&appKey=2406394919"

resp1 = sesssion.get(url1)

resp_json = resp1.json()

resp1.close()

# print(resp1_json)

#注意以上过程利用在页面中得到的cookie设置为session参数同样可以实现相应的功能。
```

### 5.2 解决防盗链

在提取网页信息时输入的参数里有一项是Referer，该项参数就是防盗链，处理它需要在参数headers中加上所要下载内容的referer网站，因为要获取视频的下载链接时，只传入视频地址不能得到网站内容，需要出入视频的referer。下面以梨视频为例，介绍其中的防盗链以及反反扒的处理。在点开梨视频网站后，随便点击一个视频，会有网页跳转，生成新的网页。

在梨视频的视频源代码页面和检查页面，都没有视频的播放链接，但是在点击播放之后，视频的播放页面会出现播放的链接，利用抓包工具可以找到视频地址的加密过后的地址，所以接下来的工作就是对视频链接进行解密。

先抓取抓包工具中的包含下载链接的接口，并传入相应的参数，包含user-agent和referer，其中referer是为了解决防盗链。将得到的结果处理为json文件，得到了加密过后的地址。对比加密的地址和真实的地址发现，其中的域名变化就是将真实的地址的一部分改成了时间戳。而且这一部分就是原网页的一部分内容，所以只需将改变的部分替换为原来的部分就能得到真实的地址。具体的代码如下：

```python
import requests
# 1.拿到cont—id
# 2.拿到json，congjson中进一步拿到被加密的链接地址
# 3.对链接地址进行正
# 4.拿到真实的下载路径进行下载

url = "https://www.pearvideo.com/video_1763204"

videostatus = "https://www.pearvideo.com/videoStatus.jsp?contId=1763204&mrd=0.6813827691278596"

headers ={"User-Agent":"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36 SLBrowser/8.0.0.7062 SLBChan/25",

#防盗链:溯源，当前请求的上一级，保证有正确的主地址
"Referer": "https://www.pearvideo.com/video_1763204"
}

#以下代码截取url中的id内容
contid = url.split("_")[1]

videostatus = f"https://www.pearvideo.com/videoStatus.jsp?contId={contid}&mrd=0.6813827691278596"

resp = requests.get(videostatus,headers=headers)

dic = resp.json()

# print(dic)

srcUrl = dic['videoInfo']['videos']['srcUrl']

systemtime = dic['systemTime']


# 真实的地址：    https://video.pearvideo.com/mp4/adshort/20220524/cont-1763204-15884803_adpkg-ad_hd.mp4
# 被加密后的地址：https://video.pearvideo.com/mp4/adshort/20220524/1657675514162-15884803_adpkg-ad_hd.mp4

srcUrl = srcUrl.replace(systemtime,f"cont-{contid}")

print(srcUrl)

#下载视频
with open('output/li.mp4',mode = 'wb') as f:
    f.write(requests.get(srcUrl).content)
```

### 5.3 设置代理

设置代理的目的就是利用其他的ip地址来代替自己的ip地址，防止由于不正确的操作而导致ip地址被封的危险。谨慎使用。ip地址的来源可以是免费ip地址网站，或者是购买。使用方式如下：

1. 声明ip地址参数。
2. 在访问网站时用proxies加上参数。

```python
# p中包括ip：183.147.29.33  端口：9000
p = {
    "https":"183.147.29.33:9000"
}
resp = requests.get("https://baidu.com",proxies=p)
resp.encoding = "utf-8"
print(resp.text)
resp.close()
```



-----

## 6. selenium

>  这是一个调用浏览器的driver，通过这个库你可以直接调用浏览器完成某些操作，比如输入验证码。Selenium是自动化测试工具，如果在这些浏览器里面安装一个 Selenium 的插件，可以方便地实现Web界面的测试. Selenium支持浏览器驱动。

利用selenium可以很好地实现网络爬虫的功能。

**[常用指令大全](https://blog.csdn.net/weixin_36279318/article/details/79475388)**

### 6.1 基础知识

1. selenium的元素定位方式

| 方式                                         | 说明                  |
| -------------------------------------------- | --------------------- |
| find_element_by_id                           | 通过元素id定位        |
| find_element_by_name                         | 通过元素name定位      |
| find_element_by_xpath                        | 通过xpath表达式定位   |
| find_element_by_link_text                    | 通过完整超链接定位    |
| find_element_by_partial_link_text            | 通过部分链接定位      |
| find_element_by_tag_name                     | 通过标签定位          |
| find_element_by_class_name                   | 通过类名进行定位      |
| find_elements_by_css_selector                | 通过css选择器进行定位 |
| 以上所有的选择方式若加上elements就是选择多个 |                       |

2. 常用操作方法

| **方法**                | **说明**                          |
| ----------------------- | --------------------------------- |
| set_window_size()       | 设置浏览器的大小                  |
| **back()**              | 控制浏览器后退，同点击‘<’         |
| **forward()**           | 控制浏览器前进，同点击‘>’         |
| refresh()               | 刷新当前页面                      |
| clear()                 | 清除文本                          |
| **send_keys (value)**   | 模拟按键输入，可以输入enter等指令 |
| **click()**             | 单击元素                          |
| submit()                | 用于提交表单                      |
| **get_attribute(name)** | 获取元素属性值                    |
| is_displayed()          | 设置该元素是否用户可见            |
| size                    | 返回元素的尺寸                    |
| **text**                | 获取元素的文本                    |

3. 窗口切换

| 指令                                         | 说明                                     |
| -------------------------------------------- | ---------------------------------------- |
| current_window_handle                        | 获得当前窗口句柄                         |
| window_handles                               | 返回所有窗口的句柄到当前会话             |
| switch_to.window()                           | 用于切换到相应的窗口，用于不同窗口的切换 |
| web.switch_to.window(web.window_handles[-1]) | 表示切换到新打开的窗口                   |

### 6.2 注意事项



-----

## 7. 多线程、多进程与协程

> - CPU 密集型：程序比较偏重于计算，需要经常使用CPU来运算。例如科学计算的程序，机器学习的程序等。
> - I/O 密集型：顾名思义就是程序需要频繁进行输入输出操作。爬虫程序就是典型的I/O密集型程序。
>
> 如果程序是属于CPU密集型，建议使用多进程。而多线程就更适合应用于I/O密集型程序。

### 7.1 线程池与进程池

在python爬虫中，使用多线程与多进程可以通过线程池与进程池来实现，线程池或进程池会自动地创建相应的线程或进程，以达到自动管理的目的。两者在python中的使用方式几乎一致。使用过程如下：

```python
#线程池和进程池，导包
from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor
# ThreadPoolExecutor是线程池
# ProcessPoolExecutor是进程池

# 多线程/进程的实现
def fun(name): #写出单个任务
    for i in range(10):
        print(name,i)
     
if __name__ == '__main__':
    #创建线程池
    with ThreadPoolExecutor(10) as t:
        for i in range(100):
            t.submit(fun,name = f"线程{i}")
            
    #with中的内容会等待线程池中的任务全部执行完毕，才能继续执行接下来的内容（守护）
    print('123')
```

使用过程总结如下：

1. 导包
2. 写出单个的任务，即函数，这些函数往往是需要被多次执行的比如需要下载很多图片，一张图片的下载就是该函数。
3. 在main中创建线程或进程池，创建方式如上。
4. 在循环中提交任务并传入每一次任务的参数。

### 7.2 协程

> 在实现多任务时, 线程切换从系统层面远不止保存和恢复 CPU上下文这么简单。 操作系统为了程序运行的高效性每个线程都有自己缓存Cache等等数据，操作系统还会帮你做这些数据的恢复操作。 所以线程的切换非常耗性能。但是协程的切换只是单纯的操作CPU的上下文，所以一秒钟切换个上百万次系统都抗的住。

以下是python中协程的模板使用：

```python
import asyncio

async def download(url):
    print("准备开始下载")
    await asyncio.sleep(2)  #异步的网络请求
    print("下载完成")

async def main():
    urls = [
        "http/:www.baidu.com",
        "http://www.bilibili.com",
        "http://www.163.com"
    ]

    tasks = []
    for url in urls:
        d = download(url) #创建协程对象
        tasks.append(asyncio.create_task(d))

    await asyncio.wait(tasks)

if __name__ =="__main__":
    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())
```

使用时要注意以下几点：

1. 创建main函数，在main函数中提交任务，并且加入时间序列即用await修饰。
2. 如果函数使用了async进行修饰，那么这个函数所创建出来的就是协程对象，不能够被直接调用或执行。
3. 在main函数中需要创建任务列表，并且任务添加时需要用到`asyncio.creat_task`修饰。这样，任务列表中的元素都是由携程对象创建的任务。
4. 最后提交任务时的函数为上面的最后两行，直接用asyncio.run会报错。
5. 协程的效率比创建线程池或进程池效率要高。

